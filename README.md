# Lower Toxicity using PPO and RLHF with Hate-speech Reward Model

This project involves the reduction of toxicity in text generation models using Reinforcement Learning from Human Feedback (RLHF) and Proximal Policy Optimization (PPO). A Hate-speech Reward Model is implemented to guide RLHF. The project aims to achieve lower toxicity while maintaining text generation quality.

The "Reinforcement Learning from Human Feedback (RLHF)" project is a significant step forward in advancing the capabilities of artificial intelligence systems. In this endeavor, we explore the transformative potential of RLHF techniques to train AI models more effectively by leveraging human guidance.


Beyond the technical aspects, we showcase results and reports that highlight the impact of RLHF on AI model performance, validating its significance. 


## Table of Contents

- [Introduction](#introduction)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Code Structure](#project-structure)
- [How to Run](#how-to-run)
- [Results](#results)
- [References](#references)
- [Author](#author)
- [License](#license)

## Introduction

The project focuses on reducing the toxicity of text generation models using RLHF and PPO. The Hate-speech Reward Model provides guidance during reinforcement learning to encourage less toxic output while preserving natural language generation quality.

## Getting Started

### Prerequisites

Before using this project, make sure you have the following prerequisites:

- Python 3.x
- PyTorch
- Transformers library
- Hugging Face Datasets library
- Accelerate library

You can install these libraries using the following commands:

```bash
! pip install transformers
! pip install datasets
! pip install torch
! pip install torchdata
! pip install accelerate -U
! pip install evaluate
! pip install rouge_score
! pip install loralib
! pip install peft
! pip install trl
```

### Installation


Clone the project repository:

```bash
git clone https://github.com/yourusername/toxicity-reduction-rlhf.git
cd toxicity-reduction-rlhf
```

## Project Structure

The project code is organized into multiple modules, each serving a specific purpose:

    - data_loader.py: This module is responsible for data loading and preparation. It includes functions to load and preprocess datasets, perform data transformations, and create data pipelines to feed into machine learning models.

    - models.py: In this module, you'll find the definitions of machine learning or deep learning models. It includes model architectures, layers, and configurations. This module is where you define the neural networks or other modeling approaches used in your project.

    - trainer.py: The trainer module is dedicated to training machine learning models. It includes training loops, loss functions, and optimization algorithms. This module manages the training process and fine-tuning of models.

    - evaluation.py: This module handles the evaluation of model performance and other relevant metrics. It includes functions for assessing model accuracy, precision, recall, F1-score, and other evaluation metrics. It's where you analyze how well your models are performing.

    - main.py: The main module serves as the entry point for your project. It ties together the data loading, model definition, training, evaluation, and other components of your project. This is where you execute the entire pipeline, define configuration parameters, and control the overall flow of your machine learning or deep learning project.


Additionally the Jupypter Notebook is provided to understand the basic flow of the process.




## How to Run

Configure the project by specifying the model name, dataset name, and other parameters in the main.py file.

Run main.py to execute the entire pipeline:

```bash
python main.py
```

## Results

The objective of this project was to reduce the toxicity of text generated by a language model. The project utilized Reinforcement Learning from Human Feedback (RLHF) techniques, in conjunction with a Hate-speech Reward Model, to guide the fine-tuning process. The results demonstrate a significant reduction in the toxicity of generated text, as measured by average toxicity scores and their standard deviations.

Toxicity Metrics Before RLHF:

    Average Toxicity: 0.0291
    Standard Deviation: 0.0797

Toxicity Metrics After RLHF:

    Average Toxicity: 0.0120
    Standard Deviation: 0.0153

### Summary of Results:

Before applying RLHF, the average toxicity score of generated text was approximately 0.0291, with a relatively high standard deviation of 0.0797. This indicated a wide variation in toxicity levels, with some outputs being highly toxic.

After implementing RLHF with the Hate-speech Reward Model, a remarkable reduction in toxicity was achieved. The average toxicity score decreased to approximately 0.0120, and the standard deviation also reduced substantially to 0.0153. These results signify a significant improvement in the quality of generated text, with notably lower levels of toxicity.

The findings suggest that RLHF, when guided by a specialized reward model, can effectively mitigate toxicity in text generation models. This reduction in toxicity is crucial for ensuring that AI-generated content is respectful, ethical, and safe for a wide range of applications, including social media, chatbots, and content generation.

The success of this project highlights the potential of RLHF techniques in improving the ethical and responsible use of AI-generated content.

For more detailed analysis and data, please refer to the project's results and reports.

## References:

- [Secrets of RLHF in Large Language Models](https://arxiv.org/abs/2307.04964)
- [RLHF For High-Performance Decision-Making: Strategies and Optimization](https://www.analyticsvidhya.com/blog/2023/09/rlhf-for-high-performance-decision-making-strategies/)


## Author

Alberto Caballero
Email: girianova@yahoo.es

## License

This project is licensed under the MIT License. See the LICENSE.md file for details.


